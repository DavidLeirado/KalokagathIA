Modelo BERT para griego y c√≥mo funciona:

https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1
https://github.com/nlpaueb/greek-bert
https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification
https://www.youtube.com/watch?v=iDulhoQ2pro
https://www.youtube.com/watch?v=-9evrZnBorM
https://arxiv.org/pdf/1810.04805.pdf


Ilustrated BERT, ilustrated transformer:

https://jalammar.github.io/illustrated-bert/
https://jalammar.github.io/illustrated-transformer/


BERT Basico y transformers

https://arxiv.org/pdf/1706.03762.pdf
https://towardsdatascience.com/bert-technology-introduced-in-3-minutes-2c2f9968268c
https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8

Preprocesado del texto para introducir en el modelo:

https://huggingface.co/transformers/preprocessing.html


Para entender tokenizadores:

https://blog.floydhub.com/tokenization-nlp/
https://www.youtube.com/watch?v=zjaRNfvNMTs


Para analizar resultados:

https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062
